\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=blue,
	filecolor=black,
	linkcolor=red,
	urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}
	
	\title{Project Title: System Verification and Validation Plan for 
	\progname{}} 
	\author{\authname}
	\date{\today}
	
	\maketitle
	
	\pagenumbering{roman}
	
	\section{Revision History}
	
	\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
		\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
		\midrule
		Date 1 & 1.0 & Notes\\
		Date 2 & 1.1 & Notes\\
		\bottomrule
	\end{tabularx}
	
	\newpage
	
	\tableofcontents
	
	\listoftables
	\wss{Remove this section if it isn't needed}
	
	\listoffigures
	\wss{Remove this section if it isn't needed}
	
	\newpage
	
	\section{Symbols, Abbreviations and Acronyms}
	
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{l l} 
		\toprule		
		\textbf{symbol} & \textbf{description}\\
		\midrule 
		T & Test\\
		\bottomrule
	\end{tabular}\\
	
	\wss{symbols, abbreviations or acronyms --- you can simply reference the SRS
		\citep{SRS} tables, if appropriate}
	
	\wss{Remove this section if it isn't needed}
	
	\newpage
	
	\pagenumbering{arabic}
	
	This document ... \wss{provide an introductory blurb and roadmap of the
		Verification and Validation plan}
	
	\section{General Information}
	
	\subsection{Summary}
	
	\wss{Say what software is being tested.  Give its name and a brief overview 
	of
		its general functions.}
	
	\subsection{Objectives}
	
	\wss{State what is intended to be accomplished.  The objective will be 
	around
		the qualities that are most important for your project.  You might have
		something like: ``build confidence in the software correctness,''
		``demonstrate adequate usability.'' etc.  You won't list all of the 
		qualities,
		just those that are most important.}
	
	\subsection{Relevant Documentation}
	
	\wss{Reference relevant documentation.  This will definitely include your 
	SRS
		and your other project documents (design documents, like MG, MIS, 
		etc).  You
		can include these even before they are written, since by the time the 
		project
		is done, they will be written.}
	
	\citet{SRS}
	
	\section{Plan}
	
	\wss{Introduce this section.   You can provide a roadmap of the sections to
		come.}
	
	\subsection{Verification and Validation Team}
	
	\wss{Your teammates.  Maybe your supervisor.
		You shoud do more than list names.  You should say what each person's 
		role is
		for the project's verification.  A table is a good way to summarize 
		this information.}
	
	\subsection{SRS Verification Plan}
	
	\wss{List any approaches you intend to use for SRS verification.  This may 
	include
		ad hoc feedback from reviewers, like your classmates, or you may plan 
		for 
		something more rigorous/systematic.}
	
	\wss{Maybe create an SRS checklist?}
	
	\subsection{Design Verification Plan}
	
	\wss{Plans for design verification}
	
	\wss{The review will include reviews by your classmates}
	
	\wss{Create a checklists?}
	
	\subsection{Verification and Validation Plan Verification Plan}
	
	\wss{The verification and validation plan is an artifact that should also 
	be verified.}
	
	\wss{The review will include reviews by your classmates}
	
	\wss{Create a checklists?}
	
	\subsection{Implementation Verification Plan}
	
	\wss{You should at least point to the tests listed in this document and the 
	unit
		testing plan.}
	
	\wss{In this section you would also give any details of any plans for 
	static verification of
		the implementation.  Potential techniques include code walkthroughs, 
		code
		inspection, static analyzers, etc.}
	
	\subsection{Automated Testing and Verification Tools}
	
	\wss{What tools are you using for automated testing.  Likely a unit testing
		framework and maybe a profiling tool, like ValGrind.  Other possible 
		tools
		include a static analyzer, make, continuous integration tools, test 
		coverage
		tools, etc.  Explain your plans for summarizing code coverage metrics.
		Linters are another important class of tools.  For the programming 
		language
		you select, you should look at the available linters.  There may also 
		be tools
		that verify that coding standards have been respected, like flake9 for
		Python.}
	
	\wss{If you have already done this in the development plan, you can point to
		that document.}
	
	\wss{The details of this section will likely evolve as you get closer to the
		implementation.}
	
	\subsection{Software Validation Plan}
	
	\wss{If there is any external data that can be used for validation, you 
	should
		point to it here.  If there are no plans for validation, you should 
		state that
		here.}
	
	\wss{You might want to use review sessions with the stakeholder to check 
	that
		the requirements document captures the right requirements.  Maybe task 
		based
		inspection?}
	
	\wss{This section might reference back to the SRS verification section.}
	
	\section{System Test Description}
	
	\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.}

The functional requirements of the system will be broken down into general phases of the system's functionality: Image Processing, API Calls, Reading and Writing to Database, and Displaying to User Interface. These subsections cover the functional requirements outlined in the SRS document.

\subsubsection{Image Processing}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}

The following set of tests correlate FR1, FR2, and FR3 in the SRS document. Manual testing will be used, in which the tester will upload images according to the test plans to validate the proper response to the input. Automated testing will be used to verify the system's image identification process.
		
\paragraph{Image Upload}

\begin{enumerate}

\item{image-upload-1\\}

Control: Manual
					
Initial State: No image in the system.
					
Input: An image file of extension .png will be uploaded into the system.
					
Output: The system will contain the uploaded image of type .png.

Test Case Derivation: The system should save the inputted image, so that further processing can be done on the image.
					
How test will be performed: An image of type .png will be saved on the tester's device. The tester will upload this image to the system.
					
\item{image-upload-2\\}

Control: Manual

Initial State: No image in the system.

Input: An image file of extension .jpg will be uploaded into the system.

Output: The system will contain the uploaded image of type .jpg.

Test Case Derivation: The system should save the inputted image, so that further processing can be done on the image.

How test will be performed: An image of type .jpg will be saved on the tester's device. The tester will upload this image to the system.

\item{image-upload-3\\}

Control: Manual

Initial State: No image in the system.

Input: An image file of extension .jpeg will be uploaded into the system.

Output: The system will contain the uploaded image of type .jpeg.

Test Case Derivation: The system should save the inputted image, so that further processing can be done on the image.

How test will be performed: An image of type .jpeg will be saved on the tester's device. The tester will upload this image to the system.

\item{image-upload-4\\}

Control: Manual

Initial State: No image in the system.

Input: A file of extension .txt will be uploaded into the system.

Output: The system will not contain the uploaded file. An error message will be returned.

Test Case Derivation: The system should only save the inputted image if it is of the proper file extension.

How test will be performed: A file of type .txt will be saved on the tester's device. The tester will upload this file to the system.

\end{enumerate}

\paragraph{Image Identification}

\begin{enumerate}
	
	\item{image-identification-1\\}
	
	Control: Automatic
	
	Initial State: System contains an image of extension .png type.
	
	Input: System is prompted to process the image.
	
	Output: The type of food that is captured in the image is returned as a string.
	
	Test Case Derivation: The system should identify the food present in an image, and return the name of the food item as a string.
	
	How test will be performed: The system identified item will be verified that it matches the true item contained in the image.
	
	\item{image-identification-2\\}

	Control: Automatic
	
	Initial State: System contains an image of extension .jpg type.
	
	Input: System is prompted to process the image.
	
	Output: The type of food that is captured in the image is returned as a string.
	
	Test Case Derivation: The system should identify the food present in an image, and return the name of the food item as a string.
	
	How test will be performed: The system identified item will be verified that it matches the true item contained in the image.
	
	\item{image-identification-3\\}
	
	Control: Automatic
	
	Initial State: System contains an image of extension .jpeg type.
	
	Input: System is prompted to process the image.
	
	Output: The type of food that is captured in the image is returned as a string.
	
	Test Case Derivation: The system should identify the food present in an image, and return the name of the food item as a string.
	
	How test will be performed: The system identified item will be verified that it matches the true item contained in the image.
	
\end{enumerate}

\subsubsection{API Calls}

These tests cover  FR4 and FR5 in the SRS document. Automated unit tests will be used to verify that the system is able to send requests to the external API.

\paragraph{Call and Fetch API Response}

\begin{enumerate}
	
	\item{api-1\\}
	
	Control: Automatic
	
	Initial State: System on standby.
	
	Input: Food item name as a string.
	
	Output: JSON containing nutirition facts for the inputted food item.
	
	Test Case Derivation: A request will be made to the Nutritionix API with the food item name, which will return a response body containing the food item's nutrition facts.
	
	How test will be performed: The API response will be verified that the contents are as expected.
	
\end{enumerate}

\subsubsection{Reading and Writing to Database}

The following tests cover  FR6 and FR7 in the SRS document. Automated unit tests will be used to verify that the system is log and read from the database.

\paragraph{Logging Data}

\begin{enumerate}
	
	\item{log-data-1\\}
	
	Control: Automatic
	
	Initial State: System contains nutritional data of a food item.
	
	Input: Request to log the data to the database.
	
	Output: Database is updated as the system writes the data to the database.
	
	Test Case Derivation: Nutritional data of a food will be saved for future reference. This is done by logging the data to the database.
	
	How test will be performed: After the system receives the request to update the database, the database will be verified that it is updated with the new data.
	
\end{enumerate}

\paragraph{Reading Data}

\begin{enumerate}
	
	\item{read-data-1\\}
	
	Control: Automatic
	
	Initial State: Database contains nutritional data of a food item.
	
	Input: Request to fetch the data of a particular food item.
	
	Output: Nutritional data of the requested food item.
	
	Test Case Derivation: The system shall be able to fetch previously recorded data.
	
	How test will be performed: After the system receives the request to fetch the data of a food item, the returned data will be verified that it matches with the saved data in the database.
	
	\item{read-data-2\\} return data of range
	
	Control: Automatic
	
	Initial State: Database contains nutritional data of 5 different dates.
	
	Input: Request to fetch the data from the last 3 most recent dates.
	
	Output: A list of the nutritional data from the last 3 most recent dates.
	
	Test Case Derivation: The system shall be able to fetch previously recorded data. The system shall be able to fetch data from a range of dates.
	
	How test will be performed: The nutritional data from the 3 most recent dates will be verified if it matches the data in the database.
	
	\item{read-data-3\\}
	
	Control: Automatic
	
	Initial State: Database contains nutritional data.
	
	Input: Request to fetch the data of a particular food item that is not logged in the database.
	
	Output: No data is returned.
	
	Test Case Derivation: The system shall be able to fetch previously recorded data. This is an edge case for if the system is requested to read data from the database that does not exist. In this case, no data will be returned.
	
	How test will be performed: The output will be verified to be empty.
	
\end{enumerate}

\subsubsection{Displaying to User Interface}

The following tests cover  FR8 and FR9 in the SRS document. Automated unit tests will be used to validate the generated visual display.

\paragraph{Visualize Data}

\begin{enumerate}
	
	\item{visualize-1\\}
	
	Control: Automatic
	
	Initial State: Nutritional data of a food item is contained in the system.
	
	Input: Request to display the nutritional information of a food item.
	
	Output: System will generate the HTML body to be displayed on the user interface.
	
	Test Case Derivation: The nutritional data of a particular food item shall be displayed on the user interface. The system will generate the frontend script to display this data.
	
	How test will be performed: The generated HTML body will be verified that it matches the expected output.
	
	\item{visualize-2\\}
	
	Control: Automatic
	
	Initial State: Database contains nutritional data of 5 different dates.
	
	Input: Request to display the past 5 dates of nutritional data in a graph.
	
	Output: System will generate the HTML body to be displayed on the user interface.
	
	Test Case Derivation: The history of logged nutritional data shall be displayed in a graph on the user interface. The system will generate the frontend script to display this data.
	
	How test will be performed: The generated HTML body will be verified that it matches the expected output.
	
\end{enumerate}
	
	\subsection{Tests for Nonfunctional Requirements}
	
	\wss{The nonfunctional requirements for accuracy will likely just reference 
	the
		appropriate functional tests from above.  The test cases should mention
		reporting the relative error for these tests.  Not all projects will
		necessarily have nonfunctional requirements related to accuracy}
	
	\wss{Tests related to usability could include conducting a usability test 
	and
		survey.  The survey will be in the Appendix.}
	
	\wss{Static tests, review, inspections, and walkthroughs, will not follow 
	the
		format for the tests given below.}
	
	\subsubsection{Area of Testing1}
	
	\paragraph{Title for Test}
	
	\begin{enumerate}
		
		\item{test-id1\\}
		
		Type: Functional, Dynamic, Manual, Static etc.
		
		Initial State: 
		
		Input/Condition: 
		
		Output/Result: 
		
		How test will be performed: 
		
		\item{test-id2\\}
		
		Type: Functional, Dynamic, Manual, Static etc.
		
		Initial State: 
		
		Input: 
		
		Output: 
		
		How test will be performed: 
		
	\end{enumerate}
	
	\subsubsection{Area of Testing2}
	
	...
	
	\subsection{Traceability Between Test Cases and Requirements}
	
	\wss{Provide a table that shows which test cases are supporting which
		requirements.}
	
	\section{Unit Test Description}
	
	\wss{Reference your MIS (detailed design document) and explain your overall
		philosophy for test case selection.}  
	\wss{This section should not be filled in until after the MIS (detailed 
	design
		document) has been completed.}
	
	\subsection{Unit Testing Scope}
	
	\wss{What modules are outside of the scope.  If there are modules that are
		developed by someone else, then you would say here if you aren't 
		planning on
		verifying them.  There may also be modules that are part of your 
		software, but
		have a lower priority for verification than others.  If this is the 
		case,
		explain your rationale for the ranking of module importance.}
	
	\subsection{Tests for Functional Requirements}
	
	\wss{Most of the verification will be through automated unit testing.  If
		appropriate specific modules can be verified by a non-testing based
		technique.  That can also be documented in this section.}
	
	\subsubsection{Module 1}
	
	\wss{Include a blurb here to explain why the subsections below cover the 
	module.
		References to the MIS would be good.  You will want tests from a black 
		box
		perspective and from a white box perspective.  Explain to the reader 
		how the
		tests were selected.}
	
	\begin{enumerate}
		
		\item{test-id1\\}
		
		Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
			be automatic}
		
		Initial State: 
		
		Input: 
		
		Output: \wss{The expected result for the given inputs}
		
		Test Case Derivation: \wss{Justify the expected value given in the 
		Output field}
		
		How test will be performed: 
		
		\item{test-id2\\}
		
		Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
			be automatic}
		
		Initial State: 
		
		Input: 
		
		Output: \wss{The expected result for the given inputs}
		
		Test Case Derivation: \wss{Justify the expected value given in the 
		Output field}
		
		How test will be performed: 
		
		\item{...\\}
		
	\end{enumerate}
	
	\subsubsection{Module 2}
	
	...
	
	\subsection{Tests for Nonfunctional Requirements}
	
	\wss{If there is a module that needs to be independently assessed for
		performance, those test cases can go here.  In some projects, planning 
		for
		nonfunctional tests of units will not be that relevant.}
	
	\wss{These tests may involve collecting performance data from previously
		mentioned functional tests.}
	
	\subsubsection{Module ?}
	
	\begin{enumerate}
		
		\item{test-id1\\}
		
		Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
			be automatic}
		
		Initial State: 
		
		Input/Condition: 
		
		Output/Result: 
		
		How test will be performed: 
		
		\item{test-id2\\}
		
		Type: Functional, Dynamic, Manual, Static etc.
		
		Initial State: 
		
		Input: 
		
		Output: 
		
		How test will be performed: 
		
	\end{enumerate}
	
	\subsubsection{Module ?}
	
	...
	
	\subsection{Traceability Between Test Cases and Modules}
	
	\wss{Provide evidence that all of the modules have been considered.}
	
	\bibliographystyle{plainnat}
	
	\bibliography{../../refs/References}
	
	\newpage
	
	\section{Appendix}
	
	This is where you can place additional information.
	
	\subsection{Symbolic Parameters}
	
	The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
	Their values are defined in this section for easy maintenance.
	
	\subsection{Usability Survey Questions?}
	
	\wss{This is a section that would be appropriate for some projects.}
	
	\newpage{}
	\section*{Appendix --- Reflection}
	
	The information in this section will be used to evaluate the team members 
	on the
	graduate attribute of Lifelong Learning.  Please answer the following 
	questions:
	
	\newpage{}
	\section*{Appendix --- Reflection}
	
	The information in this section will be used to evaluate the team members 
	on the
	graduate attribute of Lifelong Learning.  Please answer the following 
	questions:
	
	\begin{enumerate}
		\item What knowledge and skills will the team collectively need to 
		acquire to
		successfully complete the verification and validation of your project?
		Examples of possible knowledge and skills include dynamic testing 
		knowledge,
		static testing knowledge, specific tool usage etc.  You should look to
		identify at least one item for each team member.
		\item For each of the knowledge areas and skills identified in the 
		previous
		question, what are at least two approaches to acquiring the knowledge or
		mastering the skill?  Of the identified approaches, which will each team
		member pursue, and why did they make this choice?
	\end{enumerate}
	
\end{document}